{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46cd8482",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "\n",
    "import torchvision.transforms as tfs\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "\n",
    "import random\n",
    "\n",
    "from src.YoloCigaretteDataset import get_pos_neg_files, YoloCigaretteDataset\n",
    "from src.visualizeRandomSamples import visualize_random_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b94a521",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2873d191",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Всего доступных (train): pos = 5088 neg = 219\n",
      "Всего доступных (val):   pos = 257 neg = 13\n",
      "Финально для train взято: 2219 (pos,neg) = 2000 219\n",
      "Финально для val   взято: 263 (pos,neg) = 250 13\n"
     ]
    }
   ],
   "source": [
    "# Параметры (можно менять) \n",
    "TRAIN_IMG_DIR = \"Dataset/train/images\"\n",
    "TRAIN_LABEL_DIR = \"Dataset/train/labels\"\n",
    "\n",
    "VAL_IMG_DIR = \"Dataset/val/images\"\n",
    "VAL_LABEL_DIR = \"Dataset/val/labels\"\n",
    "\n",
    "# сколько хотим взять\n",
    "TRAIN_POS_N = 2000\n",
    "TRAIN_NEG_N = 2000\n",
    "\n",
    "VAL_POS_N = 250\n",
    "VAL_NEG_N = 250\n",
    "\n",
    "BATCH_SIZE = 4\n",
    "KEEP_CLASS = 0  # класс сигареты\n",
    "\n",
    "# Получаем списки для train и val\n",
    "train_pos_all, train_neg_all = get_pos_neg_files(TRAIN_IMG_DIR, TRAIN_LABEL_DIR, keep_class=KEEP_CLASS)\n",
    "val_pos_all, val_neg_all     = get_pos_neg_files(VAL_IMG_DIR, VAL_LABEL_DIR, keep_class=KEEP_CLASS)\n",
    "\n",
    "print(\"Всего доступных (train): pos =\", len(train_pos_all), \"neg =\", len(train_neg_all))\n",
    "print(\"Всего доступных (val):   pos =\", len(val_pos_all),   \"neg =\", len(val_neg_all))\n",
    "\n",
    "# Перемешиваем и режем по нужному кол-ву (по факту можно добавить проверку на длину)\n",
    "random.shuffle(train_pos_all)\n",
    "random.shuffle(train_neg_all)\n",
    "random.shuffle(val_pos_all)\n",
    "random.shuffle(val_neg_all)\n",
    "\n",
    "train_pos = train_pos_all[:TRAIN_POS_N]\n",
    "train_neg = train_neg_all[:TRAIN_NEG_N]\n",
    "\n",
    "val_pos = val_pos_all[:VAL_POS_N]\n",
    "val_neg = val_neg_all[:VAL_NEG_N]\n",
    "\n",
    "train_files = train_pos + train_neg\n",
    "val_files = val_pos + val_neg\n",
    "\n",
    "random.shuffle(train_files)\n",
    "random.shuffle(val_files)\n",
    "\n",
    "print(\"Финально для train взято:\", len(train_files), \"(pos,neg) =\", len(train_pos), len(train_neg))\n",
    "print(\"Финально для val   взято:\", len(val_files),   \"(pos,neg) =\", len(val_pos),   len(val_neg))\n",
    "\n",
    "transforms_train = tfs.Compose([\n",
    "    tfs.RandomResizedCrop(64, scale=(0.8, 1.0)),  \n",
    "    tfs.RandomHorizontalFlip(p=0.5),\n",
    "    tfs.RandomRotation(10),\n",
    "    tfs.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    tfs.ToTensor(),\n",
    "    tfs.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "transforms_val = tfs.Compose([\n",
    "    tfs.ToTensor(),\n",
    "    tfs.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "#Создаём датасеты и лоадеры \n",
    "train_dataset = YoloCigaretteDataset(TRAIN_IMG_DIR, TRAIN_LABEL_DIR, train_files, transform=transforms_train, keep_class=KEEP_CLASS, RCNN=True)\n",
    "val_dataset   = YoloCigaretteDataset(VAL_IMG_DIR,   VAL_LABEL_DIR,   val_files,   transform=transforms_val, keep_class=KEEP_CLASS, RCNN=True)\n",
    "\n",
    "train_loader = data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True,  collate_fn=lambda batch: tuple(zip(*batch)))\n",
    "val_loader   = data.DataLoader(val_dataset,   batch_size=BATCH_SIZE, shuffle=False, collate_fn=lambda batch: tuple(zip(*batch)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df743c11",
   "metadata": {},
   "source": [
    "## Бэкбон модели - глаз модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e7cc000c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class backbone(nn.Module):\n",
    "    \n",
    "    def __init__(self, pretrained=True, trainable_layers=2):\n",
    "        super().__init__()\n",
    "        weights = ResNet50_Weights.IMAGENET1K_V2 if pretrained else None # подгружаем весас ResNet50, обученные на ImageNet\n",
    "        r = resnet50(weights=weights) # Задаем веса модели ResNet\n",
    "        \n",
    "        self.body = nn.Sequential( # Берем всю внутрянку, без классификатора - нам нужен только расспознаватель образов\n",
    "            r.conv1, \n",
    "            r.bn1,\n",
    "            r.relu,\n",
    "            r.maxpool,\n",
    "            r.layer1,\n",
    "            r.layer2,\n",
    "            r.layer3,\n",
    "            r.layer4\n",
    "        )\n",
    "        \n",
    "        self._freeze(r, trainable_layers=trainable_layers) # Включаем обучение части слоев\n",
    "        \n",
    "        self.out_channels = 2048 # Выходное кол-во каналов\n",
    "        \n",
    "    def _freeze(self, r, trainable_layers=2):\n",
    "        \n",
    "        for p in r.parameters():\n",
    "            p.requires_grad = False # Морозит градиенты на всех слоях\n",
    "            \n",
    "        layers = [r.layer1, r.layer2, r.layer3, r.layer4]\n",
    "        for l in layers[-trainable_layers]:\n",
    "            for p in l.parameters():\n",
    "                p.requires_grad = True\n",
    "                \n",
    "    def forward(self, x):\n",
    "        return self.body(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880fa9ee",
   "metadata": {},
   "source": [
    "## Генератор якорей (acnhor) - раскладываем якоря по карте признаков"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9d79c880",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models.detection.rpn import AnchorGenerator, RPNHead, RegionProposalNetwork\n",
    "from torchvision.models.detection.image_list import ImageList\n",
    "from torchvision.ops import MultiScaleRoIAlign\n",
    "from torchvision.models.detection.roi_heads import RoIHeads\n",
    "from torchvision.models.detection.faster_rcnn import TwoMLPHead, FastRCNNPredictor\n",
    "\n",
    "\n",
    "class MyFasterRCNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_classes, roi_size=7):\n",
    "        super().__init__()\n",
    "        self.backbone = backbone()\n",
    "        self.archor_generator = AnchorGenerator(sizes=((32, 64, 128, 256, 512),), # Кортеж кортежей потому, что карт признаков может быть несколько, если мы пытаемся обобщать масштаб\n",
    "                                            aspect_ratios=((0.5, 1.0, 2.0),))\n",
    "        self.rpn_head = RPNHead(in_channels=self.backbone.out_channels, num_anchors=self.archor_generator.num_anchors_per_location()[0])\n",
    "        self.rpn = RegionProposalNetwork(\n",
    "            anchor_generator=self.archor_generator,\n",
    "            head=self.rpn_head,\n",
    "            fg_iou_thresh=0.7, # Порог пересечения, выше которого якорь считается позитивным\n",
    "            bg_iou_thresh=0.3, # Порог негативного якоря\n",
    "            batch_size_per_image=256, # сколько якорей идет в батч для вычисления loss\n",
    "            positive_fraction=0.5, # Такая доля из батча должна быть позитивной, чтобы сеть быстрее научилась определять фон\n",
    "            pre_nms_top_n={\"training\": 1000, \"testing\": 500}, # Сколько лучших боксов оставить до NMS\n",
    "            post_nms_top_n={\"training\": 300, \"testing\": 300}, # Сколько боксов оставить после NMS\n",
    "            nms_thresh=0.7, # Если между боксами IoU более 0.7, то слабый бокс будет отброшен и будет считаться частью сильного\n",
    "            score_thresh=0.0\n",
    "        )\n",
    "\n",
    "        self.roi_size = roi_size\n",
    "        self.box_roi_pool = MultiScaleRoIAlign(\n",
    "            featmap_names=['0'],\n",
    "            output_size=self.roi_size,\n",
    "            sampling_ratio=2\n",
    "        ) # Вырезаем соотвествующую область на карте признаков, приводим к фикс. размеру\n",
    "        \n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        representation_size = 1024\n",
    "        box_head_in = self.backbone.out_channels * roi_size * roi_size\n",
    "        self.box_head = TwoMLPHead(in_channels=box_head_in, representation_size=representation_size) # Два полносвязных слоя\n",
    "        self.box_predictor = FastRCNNPredictor(in_channels=representation_size, num_classes=num_classes) # Решение задачи классификкации, задачи регрессии\n",
    "        \n",
    "        \n",
    "        self.roi_heads = RoIHeads(\n",
    "            box_roi_pool=self.box_roi_pool,\n",
    "            box_head=self.box_head,\n",
    "            box_predictor=self.box_predictor,\n",
    "            fg_iou_thresh=0.5,\n",
    "            bg_iou_thresh=0.5,\n",
    "            batch_size_per_image=256,\n",
    "            positive_fraction=0.25,\n",
    "            bbox_reg_weights=None,\n",
    "            score_thresh=0.05,\n",
    "            nms_thresh=0.5,\n",
    "            detections_per_img=100,\n",
    "        )\n",
    "\n",
    "    def forward(self, x, targets=None):\n",
    "\n",
    "        image_sizes = [img.shape[-2:] for img in x] # Сохранение размерностей всех картинок\n",
    "        batch_tensor = self.pad_images_to_max_size(x) # Приведение картинок к общему размеру по максимальному в батче\n",
    "\n",
    "        image_list = ImageList(batch_tensor, image_sizes) # Каждому изображению новой размерности соотвествует его старая размерность\n",
    "\n",
    "        feature_map = self.backbone(image_list.tensors) # Прогнали картинки через backbone и получили на выходе карту признаков\n",
    "        features = {'0': feature_map}\n",
    "        \n",
    "        proposals, rpn_losses = self.rpn(image_list, features, targets) # выделение потенциальных регионов, генерация якорей разных рамеров и пропорций, происходит отбор наиболее потенциальных якорей\n",
    "        detections, roi_losses = self.roi_heads(features, proposals, image_sizes, targets)\n",
    "        \n",
    "        if self.training:\n",
    "            losses = {}\n",
    "            losses.update(rpn_losses)\n",
    "            losses.update(roi_losses)\n",
    "            \n",
    "            return losses\n",
    "        \n",
    "        return detections\n",
    "    \n",
    "    @staticmethod\n",
    "    def pad_images_to_max_size(x, pad_val=0):\n",
    "        max_h = max(i.shape[-2] for i in x)\n",
    "        max_w = max(i.shape[-1] for i in x)\n",
    "        \n",
    "        batch = []\n",
    "        for i in x:\n",
    "            c, h, w = i.shape\n",
    "            padded = i.new_full((c, max_h, max_w), pad_val)\n",
    "            padded[:, :h, :w] = i\n",
    "            batch.append(padded)\n",
    "        \n",
    "        return torch.stack(batch)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f13b29de",
   "metadata": {},
   "source": [
    "# Модель PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ef87f78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection import FasterRCNN_ResNet50_FPN_Weights\n",
    "\n",
    "num_classes = 2  # фон + сигарета\n",
    "\n",
    "weights = FasterRCNN_ResNet50_FPN_Weights.DEFAULT\n",
    "model = fasterrcnn_resnet50_fpn(weights=weights)\n",
    "\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "for p in model.backbone.parameters():\n",
    "    p.requires_grad = False # морожу все параметры / отключаю обучение\n",
    "    \n",
    "for p in model.backbone.body.layer3.parameters():\n",
    "    p.requires_grad = True\n",
    "for p in model.backbone.body.layer4.parameters():\n",
    "    p.requires_grad = True\n",
    "    \n",
    "model.transform.min_size = (480, )\n",
    "model.transform.max_size = 800\n",
    "\n",
    "model.rpn.post_nms_top_n_train = 300\n",
    "model.rpn.post_nms_top_n_test = 300\n",
    "\n",
    "model.rpn.pre_nms_top_n_train = 1000\n",
    "model.rpn.pre_nms_top_n_test = 500\n",
    "\n",
    "model.roi_heads.batch_size_per_image = 128\n",
    "model.roi_heads.positive_fraction = 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "51f8b221",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Эпоха тренировочная 1/10: 100%|██████████| 555/555 [26:08<00:00,  2.83s/it, loss=tensor(0.1550, device='cuda:0', grad_fn=<AddBackward0>)]\n",
      "Эпоха валидационная 1/10: 100%|██████████| 66/66 [01:55<00:00,  1.75s/it, map=1.26e-5] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Сохранена модел с результатом MAP: 1.2628814147319645e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Эпоха тренировочная 2/10: 100%|██████████| 555/555 [06:36<00:00,  1.40it/s, loss=tensor(0.3674, device='cuda:0', grad_fn=<AddBackward0>)]\n",
      "Эпоха валидационная 2/10: 100%|██████████| 66/66 [00:56<00:00,  1.16it/s, map=0]\n",
      "Эпоха тренировочная 3/10: 100%|██████████| 555/555 [05:58<00:00,  1.55it/s, loss=tensor(0.1011, device='cuda:0', grad_fn=<AddBackward0>)]\n",
      "Эпоха валидационная 3/10: 100%|██████████| 66/66 [00:57<00:00,  1.14it/s, map=5.15e-5] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Сохранена модел с результатом MAP: 5.147828414919786e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Эпоха тренировочная 4/10: 100%|██████████| 555/555 [06:27<00:00,  1.43it/s, loss=tensor(0.1119, device='cuda:0', grad_fn=<AddBackward0>)]\n",
      "Эпоха валидационная 4/10: 100%|██████████| 66/66 [01:01<00:00,  1.08it/s, map=8.39e-5] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Сохранена модел с результатом MAP: 8.390149014303461e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Эпоха тренировочная 5/10: 100%|██████████| 555/555 [06:02<00:00,  1.53it/s, loss=tensor(56.5205, device='cuda:0', grad_fn=<AddBackward0>)]    \n",
      "Эпоха валидационная 5/10: 100%|██████████| 66/66 [00:53<00:00,  1.22it/s, map=0]\n",
      "Эпоха тренировочная 6/10: 100%|██████████| 555/555 [05:48<00:00,  1.59it/s, loss=tensor(6.8857, device='cuda:0', grad_fn=<AddBackward0>)]     \n",
      "Эпоха валидационная 6/10: 100%|██████████| 66/66 [00:55<00:00,  1.19it/s, map=0]\n",
      "Эпоха тренировочная 7/10: 100%|██████████| 555/555 [05:57<00:00,  1.55it/s, loss=tensor(0.4498, device='cuda:0', grad_fn=<AddBackward0>)] \n",
      "Эпоха валидационная 7/10: 100%|██████████| 66/66 [00:52<00:00,  1.25it/s, map=0]\n",
      "Эпоха тренировочная 8/10: 100%|██████████| 555/555 [06:05<00:00,  1.52it/s, loss=tensor(0.2328, device='cuda:0', grad_fn=<AddBackward0>)]\n",
      "Эпоха валидационная 8/10: 100%|██████████| 66/66 [00:58<00:00,  1.12it/s, map=0]\n",
      "Эпоха тренировочная 9/10: 100%|██████████| 555/555 [06:19<00:00,  1.46it/s, loss=tensor(0.2179, device='cuda:0', grad_fn=<AddBackward0>)]\n",
      "Эпоха валидационная 9/10: 100%|██████████| 66/66 [01:18<00:00,  1.19s/it, map=8.99e-7]\n",
      "Эпоха тренировочная 10/10: 100%|██████████| 555/555 [09:35<00:00,  1.04s/it, loss=tensor(0.1451, device='cuda:0', grad_fn=<AddBackward0>)]\n",
      "Эпоха валидационная 10/10: 100%|██████████| 66/66 [01:13<00:00,  1.11s/it, map=0]\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "from torchmetrics.detection.mean_ap import MeanAveragePrecision\n",
    "metric = MeanAveragePrecision(iou_thresholds=[0.5]).to(device)\n",
    "\n",
    "# model = MyFasterRCNN(2)\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "epochs = 10\n",
    "optimizer = torch.optim.AdamW(params=[p for p in model.parameters() if p.requires_grad], lr=0.001, weight_decay=0.001)\n",
    "\n",
    "best_map = 0\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    metric.reset()\n",
    "    \n",
    "    train_bar = tqdm(train_loader, desc=f'Эпоха тренировочная {epoch+1}/{epochs}', position=0)\n",
    "    \n",
    "    model.train()\n",
    "    for x_train, y_train in train_bar:\n",
    "        x_train = [x.to(device) for x in x_train]\n",
    "        y_train = [{k: v.to(device) for k, v in t.items()} for t in y_train]\n",
    "        \n",
    "        loss = sum(model(x_train, y_train).values())\n",
    "\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_bar.set_postfix({\n",
    "            'loss': loss\n",
    "        })\n",
    "\n",
    "        \n",
    "    model.eval()\n",
    "    \n",
    "    val_bar = tqdm(val_loader, desc=f'Эпоха валидационная {epoch+1}/{epochs}', position=0)\n",
    "    for x_val, y_val in val_bar:\n",
    "        \n",
    "        x_val = [x.to(device) for x in x_val]\n",
    "        y_val = [{k: v.to(device) for k, v in t.items()} for t in y_val]\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            pred = model(x_val)\n",
    "            \n",
    "        metric.update(pred, y_val)\n",
    "        res = metric.compute()\n",
    "        _map = float(res['map_50'].item())\n",
    "        \n",
    "        val_bar.set_postfix({\n",
    "            'map': _map\n",
    "        })\n",
    "\n",
    "    \n",
    "    if _map >= best_map:\n",
    "        print(f'Сохранена модел с результатом MAP: {_map}')\n",
    "        best_map = _map\n",
    "        torch.save(model.state_dict(), 'best_model_frcnn.tar')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196194fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
